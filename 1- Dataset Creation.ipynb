{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import urllib.request\n",
    "import math\n",
    "\n",
    "def get_tile_coordinates(lat1, lng1, lat2, lng2, zoom):\n",
    "    tile_size = 256\n",
    "    num_tiles_x = 1 << zoom\n",
    "    num_tiles_y = 1 << zoom\n",
    "\n",
    "    x1, y1 = get_tile(lat1, lng1, zoom)\n",
    "    x2, y2 = get_tile(lat2, lng2, zoom)\n",
    "\n",
    "    min_x, max_x = min(x1, x2), max(x1, x2)\n",
    "    min_y, max_y = min(y1, y2), max(y1, y2)\n",
    "\n",
    "    return [(x, y) for x in range(min_x, max_x + 1) for y in range(min_y, max_y + 1)]\n",
    "def get_tile(lat, lng, zoom):\n",
    "    tile_size = 256\n",
    "    num_tiles = 1 << zoom\n",
    "\n",
    "    point_x = (tile_size / 2 + lng * tile_size / 360.0) * num_tiles // tile_size\n",
    "    sin_y = math.sin(lat * (math.pi / 180.0))\n",
    "    point_y = ((tile_size / 2) + 0.5 * math.log((1 + sin_y) / (1 - sin_y)) * -(tile_size / (2 * math.pi))) * num_tiles // tile_size\n",
    "\n",
    "    return int(point_x), int(point_y)\n",
    "\n",
    "def download_tiles(coordinates, zoom, directory):\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    for x, y in coordinates:\n",
    "        lat, lng = get_lat_lng_for_tile(x, y, zoom)\n",
    "        url = f'https://mt0.google.com/vt?lyrs=s&x={x}&y={y}&z={zoom}'\n",
    "        filename = os.path.join(directory, f'tile_{lat}_{lng}_{zoom}.png')\n",
    "\n",
    "        # Download the tile\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "def get_lat_lng_for_tile(x, y, zoom):\n",
    "    tile_size = 256\n",
    "    num_tiles = 1 << zoom\n",
    "\n",
    "    lng = x / num_tiles * 360.0 - 180\n",
    "    n = math.pi - 2 * math.pi * y / num_tiles\n",
    "    lat = 180.0 / math.pi * math.atan(0.5 * (math.exp(n) - math.exp(-n)))\n",
    "\n",
    "    return lat, lng\n",
    "\n",
    "def get_dataframe(tile_coordinates, zoom):\n",
    "    # Create a dataframe to hold the coordinates\n",
    "    df = pd.DataFrame(columns=['lat', 'lng', 'filename'])\n",
    "\n",
    "    # Iterate over the tile coordinates\n",
    "    for x, y in tile_coordinates:\n",
    "        # Calculate the latitudes and longitudes of the tile\n",
    "        lat, lng = get_lat_lng_for_tile(x, y, zoom)\n",
    "        filename = f'tile_{lat}_{lng}_{zoom}.png'\n",
    "\n",
    "        # Add the coordinates and filename to the dataframe\n",
    "        df = df.append({'lat': lat, 'lng': lng, 'filename': filename}, ignore_index=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    # Coordinates for the opposite corners of the rectangular area\n",
    "    lat1, lng1 = 32.013533269028436, 35.92912375369814\n",
    "    lat2, lng2 = 31.988425707953432, 35.95037724675171\n",
    "    zoom_level = 19  # Adjust the zoom level as needed\n",
    "    output_directory = \"test3\"  # Specify the directory name\n",
    "\n",
    "    # Calculate tile coordinates\n",
    "    tile_coordinates = get_tile_coordinates(lat1, lng1, lat2, lng2, zoom_level)\n",
    "\n",
    "    # Get dataframe\n",
    "    df = get_dataframe(tile_coordinates, zoom_level)\n",
    "    df.to_csv('test3.csv')\n",
    "    # Download tiles\n",
    "    download_tiles(tile_coordinates, zoom_level, output_directory)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeling Images for PV Solar Panel Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install pyqt5 lxml\n",
    "%conda create --name=labelme python=3\n",
    "%conda activate labelme\n",
    "%pip install labelme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!labelme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Dataset\n",
    "Splitting the labeled images into a train, test, and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitter(ValTestRatio):\n",
    "  import os\n",
    "  import shutil\n",
    "  from sklearn.model_selection import train_test_split\n",
    "\n",
    "  !rm -r /content/Dataset/Dataset/Images/train\n",
    "  !rm -r /content/Dataset/Dataset/Images/val\n",
    "  !rm -r /content/Dataset/Dataset/Images/test\n",
    "\n",
    "\n",
    "  # Define your dataset directories\n",
    "  source_images_dir = '/content/Dataset/Dataset/Images'\n",
    "  source_labels_dir = '/content/Dataset/Dataset/Labels'\n",
    "  train_images_dir = '/content/Dataset/Dataset/Images/train'\n",
    "  val_images_dir = '/content/Dataset/Dataset/Images/val'\n",
    "  test_images_dir = '/content/Dataset/Dataset/Images/test'\n",
    "\n",
    "  # Create directories for train, validation, and test sets\n",
    "  os.makedirs(train_images_dir, exist_ok=True)\n",
    "  os.makedirs(val_images_dir, exist_ok=True)\n",
    "  os.makedirs(test_images_dir, exist_ok=True)\n",
    "\n",
    "  # Get all image filenames (assuming they are PNG files)\n",
    "  all_images = [f for f in os.listdir(source_images_dir) if f.endswith('.png')]\n",
    "\n",
    "  # Split the dataset into train, validation, and test sets\n",
    "  train_images, remaining_images = train_test_split(all_images, test_size=ValTestRatio*2, random_state=42)\n",
    "  val_images, test_images = train_test_split(remaining_images, test_size=0.5, random_state=42)\n",
    "\n",
    "  # Copy image files and corresponding label files to their respective directories\n",
    "  for image in train_images:\n",
    "      shutil.copy(os.path.join(source_images_dir, image), train_images_dir)\n",
    "      # Copy corresponding label file\n",
    "      label_file = image.replace('.png', '.txt')\n",
    "      shutil.copy(os.path.join(source_labels_dir, label_file), train_images_dir)\n",
    "\n",
    "  for image in val_images:\n",
    "      shutil.copy(os.path.join(source_images_dir, image), val_images_dir)\n",
    "      # Copy corresponding label file\n",
    "      label_file = image.replace('.png', '.txt')\n",
    "      shutil.copy(os.path.join(source_labels_dir, label_file), val_images_dir)\n",
    "\n",
    "  for image in test_images:\n",
    "      shutil.copy(os.path.join(source_images_dir, image), test_images_dir)\n",
    "      # Copy corresponding label file\n",
    "      label_file = image.replace('.png', '.txt')\n",
    "      shutil.copy(os.path.join(source_labels_dir, label_file), test_images_dir)\n",
    "      import os\n",
    "\n",
    "  import yaml\n",
    "\n",
    "  # Define the data\n",
    "  data = {\n",
    "      'train': '/content/Dataset/Dataset/Images/train',\n",
    "      'val': '/content/Dataset/Dataset/Images/val',\n",
    "      'test': '/content/Dataset/Dataset/Images/test',\n",
    "      'nc': 1,\n",
    "      'names': ['0']  # 'names' should be a list of class names\n",
    "  }\n",
    "\n",
    "  # Write the data to a YAML file\n",
    "  with open('/content/yolov5/dataset.yaml', 'w') as file:\n",
    "      yaml.dump(data, file)\n",
    "\n",
    "  # Define the paths to your train, val, and test folders\n",
    "  train_folder = '/content/Dataset/Dataset/Images/train'\n",
    "  val_folder = '/content/Dataset/Dataset/Images/val'\n",
    "  test_folder = '/content/Dataset/Dataset/Images/test'\n",
    "\n",
    "  # Function to count files and divide by two\n",
    "  def count_and_divide_by_two(folder):\n",
    "      file_count = len(os.listdir(folder))\n",
    "      divided_count = file_count // 2\n",
    "      return divided_count, divided_count / file_count if file_count > 0 else 0\n",
    "\n",
    "  # Count and divide by two for each folder\n",
    "  train_count, train_ratio = count_and_divide_by_two(train_folder)\n",
    "  val_count, val_ratio = count_and_divide_by_two(val_folder)\n",
    "  test_count, test_ratio = count_and_divide_by_two(test_folder)\n",
    "\n",
    "  # Print the results\n",
    "  print(f\"Number of images for train: {train_count}, Ratio: {train_count/(train_count+val_count+test_count)}\")\n",
    "  print(f\"Number of images for val: {val_count}, Ratio: {val_count/(train_count+val_count+test_count)}\")\n",
    "  print(f\"Number of images for test: {test_count}, Ratio: {test_count/(train_count+val_count+test_count)}\")\n",
    "\n",
    "  import glob\n",
    "  import os\n",
    "\n",
    "  def count_objects(path):\n",
    "    count = 0\n",
    "    for file in glob.glob(os.path.join(path, \"*.txt\")):\n",
    "      with open(file) as f:\n",
    "        count += len(f.readlines())\n",
    "    return count\n",
    "\n",
    "  print(f\"Number of objects in test set: {count_objects('/content/Dataset/Dataset/Images/test')}\")\n",
    "  print(f\"Number of objects in validation set: {count_objects('/content/Dataset/Dataset/Images/val')}\")\n",
    "  print(f\"Number of objects in training set: {count_objects('/content/Dataset/Dataset/Images/train')}\")\n",
    "  print(\"dataset.yaml file contains:\\n---------------\")\n",
    "  !cat /content/yolov5/dataset.yaml\n",
    "  print(\"---------------\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
